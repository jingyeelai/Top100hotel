{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper Workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "from typing import List, TypedDict\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "from loguru import logger as log\n",
    "from scrapfly import ScrapeApiResponse, ScrapeConfig, ScrapflyClient\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2 import extras\n",
    "from elasticsearch import Elasticsearch\n",
    "import hashlib\n",
    "import hidden\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='Unverified HTTPS request')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up database connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.9/site-packages/elasticsearch/_sync/client/__init__.py:395: SecurityWarning: Connecting to 'https://es.matrix4p.com:9200' using TLS with verify_certs=False is insecure\n",
      "  _transport = transport_class(\n"
     ]
    }
   ],
   "source": [
    "# Set up Elasticsearch connection\n",
    "secrets = hidden.elastic()\n",
    "\n",
    "es = Elasticsearch(\n",
    "    f\"https://{secrets['host']}:{secrets['port']}\",\n",
    "    ca_certs=secrets['ca'],\n",
    "    basic_auth=(secrets['user'], secrets['pass']),\n",
    "    verify_certs=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up PostgreSQL connection\n",
    "sql_string = hidden.psycopg2(hidden.postgres())\n",
    "conn = psycopg2.connect(sql_string, connect_timeout=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define scraper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_hotel(query: str, session: ScrapflyClient):\n",
    "    \"\"\"\n",
    "    search for hotel page from given query.\n",
    "    \"\"\"\n",
    "    log.info(f\"searching: {query}\")\n",
    "    url = \"https://www.tripadvisor.com/data/graphql/ids\"\n",
    "    \n",
    "    payload = json.dumps(\n",
    "        [\n",
    "            {\n",
    "                \"query\": \"c9d791589f937ec371723f236edc7c6b\",\n",
    "                \"variables\": {\n",
    "                    \"request\": {\n",
    "                        \"query\": query,\n",
    "                        \"limit\": 10,\n",
    "                        \"scope\": \"WORLDWIDE\",\n",
    "                        \"locale\": \"en-US\",\n",
    "                        \"scopeGeoId\": 1,\n",
    "                        \"searchCenter\": None,\n",
    "                        \"types\": [\n",
    "                            \"LOCATION\",\n",
    "                            \"QUERY_SUGGESTION\",\n",
    "                            \"USER_PROFILE\",\n",
    "                            \"RESCUE_RESULT\"\n",
    "                        ],\n",
    "                        \"locationTypes\": [\n",
    "                            #   \"GEO\",\n",
    "                            #   \"AIRPORT\",\n",
    "                            \"ACCOMMODATION\",\n",
    "                            #   \"ATTRACTION\",\n",
    "                            #   \"ATTRACTION_PRODUCT\",\n",
    "                            #   \"EATERY\",\n",
    "                            #   \"NEIGHBORHOOD\",\n",
    "                            #   \"AIRLINE\",\n",
    "                            #   \"SHOPPING\",\n",
    "                            #   \"UNIVERSITY\",\n",
    "                            #   \"GENERAL_HOSPITAL\",\n",
    "                            #   \"PORT\",\n",
    "                            #   \"FERRY\",\n",
    "                            #   \"CORPORATION\",\n",
    "                            #   \"VACATION_RENTAL\",\n",
    "                            #   \"SHIP\",\n",
    "                            #   \"CRUISE_LINE\",\n",
    "                            #   \"CAR_RENTAL_OFFICE\"\n",
    "                        ],\n",
    "                        \"userId\": None,\n",
    "                        \"context\": {\n",
    "                            \"searchSessionId\": \"E79EC7492FFD0BE3C904AA89F02DC1AA1665037199910ssid\",\n",
    "                            \"typeaheadId\": \"1665037209369\",\n",
    "                            \"uiOrigin\": \"SINGLE_SEARCH_HERO\",\n",
    "                            \"routeUid\": \"LIT@eBE@IIdXLLmlCycNjnyj\"\n",
    "                        },\n",
    "                        \"articleCategories\": [\n",
    "                            \"default\",\n",
    "                            \"love_your_local\",\n",
    "                            \"insurance_lander\"\n",
    "                        ],\n",
    "                        \"enabledFeatures\": [\n",
    "                            \"typeahead-q\"\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    headers = {\n",
    "        # we need to generate a random request ID for this request to succeed\n",
    "        \"content-type\": \"application/json\",\n",
    "        \"x-requested-by\": \"\".join(random.choice(string.ascii_lowercase + string.digits) for i in range(64)),\n",
    "    }\n",
    "    result = session.scrape(\n",
    "        ScrapeConfig(\n",
    "            url=url,\n",
    "            country=\"US\",\n",
    "            headers=headers,\n",
    "            body=payload,\n",
    "            method=\"POST\",\n",
    "            asp=True,\n",
    "        )\n",
    "    )\n",
    "    data = json.loads(result.content)\n",
    "\n",
    "    # return the most matched result\n",
    "    log.info(f'found {len(data[0][\"data\"][\"Typeahead_autocomplete\"][\"results\"])} results, taking the most matched result')\n",
    "    return data[0][\"data\"][\"Typeahead_autocomplete\"][\"results\"][0][\"details\"]\n",
    "    \n",
    "    # for res in data[0]['data']['Typeahead_autocomplete']['results']:\n",
    "    #     # confirming if the `localizedName` of `res` matches the query, and has `latitude` value\n",
    "    #     if (query.split(',')[0].lower() in res['details']['localizedName'].lower()) and (res['details']['latitude'] != None):\n",
    "    #         matched_result = res\n",
    "    # return matched_result[\"details\"]\n",
    "\n",
    "\n",
    "def extract_page_manifest(html):\n",
    "    \"\"\"extract javascript state data hidden in TripAdvisor HTML pages\"\"\"\n",
    "    data = re.findall(r\"pageManifest:({.+?})};\", html, re.DOTALL)[0]\n",
    "    return json.loads(data)\n",
    "\n",
    "\n",
    "def extract_named_urql_cache(urql_cache: dict, pattern: str):\n",
    "    \"\"\"extract named urql response cache from hidden javascript state data\"\"\"\n",
    "    data = json.loads(next(v[\"data\"] for k, v in urql_cache.items() if pattern in v[\"data\"]))\n",
    "    return data\n",
    "\n",
    "\n",
    "class Review(TypedDict):\n",
    "    id: str\n",
    "    date: str\n",
    "    rating: str\n",
    "    title: str\n",
    "    text: str\n",
    "    votes: int\n",
    "    url: str\n",
    "    language: str\n",
    "    platform: str\n",
    "    author_id: str\n",
    "    author_name: str\n",
    "    author_username: str\n",
    "\n",
    "\n",
    "def parse_reviews(result: ScrapeApiResponse) -> List[Review]:\n",
    "    \"\"\"Parse reviews from a review page\"\"\"\n",
    "    page_data = extract_page_manifest(result.content)\n",
    "    review_cache = extract_named_urql_cache(page_data[\"urqlCache\"], '\"reviewListPage\"')\n",
    "    parsed = []\n",
    "    # review data contains loads of information, let's parse only the basic in this tutorial\n",
    "    for review in review_cache[\"locations\"][0][\"reviewListPage\"][\"reviews\"]:\n",
    "        parsed.append(\n",
    "            {\n",
    "                \"id\": review[\"id\"],\n",
    "                \"date\": review[\"publishedDate\"],\n",
    "                \"rating\": review[\"rating\"],\n",
    "                \"title\": review[\"title\"],\n",
    "                \"text\": review[\"text\"],\n",
    "                \"votes\": review[\"helpfulVotes\"],\n",
    "                \"url\": review[\"route\"][\"url\"],\n",
    "                \"language\": review[\"language\"],\n",
    "                \"platform\": review[\"publishPlatform\"],\n",
    "                \"author_id\": review[\"userProfile\"][\"id\"],\n",
    "                \"author_name\": review[\"userProfile\"][\"displayName\"],\n",
    "                \"author_username\": review[\"userProfile\"][\"username\"],\n",
    "            }\n",
    "        )\n",
    "    return parsed\n",
    "\n",
    "\n",
    "class Hotel(TypedDict):\n",
    "    name: str\n",
    "    id: int\n",
    "    type: str\n",
    "    description: str\n",
    "    rating: int\n",
    "    rating_count: int\n",
    "    features: List[str]\n",
    "    stars: str or None\n",
    "\n",
    "\n",
    "def parse_hotel_info(data: dict) -> Hotel:\n",
    "    \"\"\"parse hotel data from TripAdvisor javascript state to something more readable\"\"\"\n",
    "    parsed = {}\n",
    "    # there's a lot of information in hotel data, in this tutorial let's extract the basics:\n",
    "    parsed[\"name\"] = data[\"name\"]\n",
    "    parsed[\"id\"] = data[\"locationId\"]\n",
    "    parsed[\"type\"] = data[\"accommodationType\"]\n",
    "    parsed[\"description\"] = data[\"locationDescription\"]\n",
    "    parsed[\"rating\"] = data[\"reviewSummary\"][\"rating\"]\n",
    "    parsed[\"rating_count\"] = data[\"reviewSummary\"][\"count\"]\n",
    "    # for hotel \"features\" lets just extract the names:\n",
    "    parsed[\"features\"] = []\n",
    "    for amenity_type, values in data[\"detail\"][\"hotelAmenities\"][\"highlightedAmenities\"].items():\n",
    "        for value in values:\n",
    "            parsed[\"features\"].append(f\"{amenity_type}_{value['amenityNameLocalized'].lower()}\")\n",
    "\n",
    "    if star_rating := data[\"detail\"][\"starRating\"]:\n",
    "        parsed[\"stars\"] = star_rating[0][\"tagNameLocalized\"]\n",
    "    return parsed  # type: ignore\n",
    "\n",
    "\n",
    "class HotelAllData(TypedDict):\n",
    "    hotel: str\n",
    "    url: str\n",
    "    info: Hotel\n",
    "    reviews: List[Review]\n",
    "    price: List[dict]\n",
    "    page: dict\n",
    "\n",
    "\n",
    "def scrape_hotel(hotel: str, url: str, session: ScrapflyClient) -> HotelAllData:\n",
    "    \"\"\"Scrape all hotel data: information, pricing and reviews\"\"\"\n",
    "    log.info(f\"scraping: {url}\")\n",
    "    first_page = session.scrape(ScrapeConfig(url=url, country=\"US\"))\n",
    "    if first_page.prevent_extra_usage():\n",
    "        print(f'{first_page.remaining_quota} left.')\n",
    "        print(f'{first_page.cost} USD per extra API call.')\n",
    "        return first_page  # type: ignore\n",
    "    page_data = extract_page_manifest(first_page.content)\n",
    "    #return page_data\n",
    "\n",
    "    # price data keys are dynamic first we need to find the full key name\n",
    "    _pricing_key = next(\n",
    "        (key for key, val in page_data['urqlCache'].items() if 'HPS_getWebHROffers' in val['data'])\n",
    "    )\n",
    "    pricing_details = list()\n",
    "    price_page = json.loads(page_data['urqlCache'][_pricing_key]['data'])['HPS_getWebHROffers']\n",
    "    for offer_key in ['chevronOffers', 'textLinkOffers']:\n",
    "        if any([source['status'] == 'AVAILABLE' for source in price_page[offer_key]]):\n",
    "            for source in price_page[offer_key]:\n",
    "                if source['status'] == 'AVAILABLE':\n",
    "                    pricing = dict()\n",
    "                    pricing['vendorName'] = source['data']['dataAtts']['vendorName']\n",
    "                    pricing['perNight'] = source['data']['dataAtts']['perNight']\n",
    "                    pricing_details.append(pricing)\n",
    "    log.info(f\"found {len(pricing_details)} prices from {[pricing['vendorName'] for pricing in pricing_details]}\")\n",
    "\n",
    "    # We can extract data from Graphql cache embeded in the page\n",
    "    # TripAdvisor is using: https://github.com/FormidableLabs/urql as their graphql client\n",
    "    try:\n",
    "        hotel_cache = extract_named_urql_cache(page_data[\"urqlCache\"], '\"locationDescription\"')\n",
    "        hotel_info = hotel_cache[\"locations\"][0]\n",
    "        info = parse_hotel_info(hotel_info)\n",
    "    except:\n",
    "        hotel_cache = json.loads(next(v[\"data\"] for k, v in page_data[\"urqlCache\"].items() \n",
    "                                    if ('\"locationDescription\"' in v[\"data\"]) and \n",
    "                                       ('\"accommodationType\"' in v[\"data\"]) and \n",
    "                                       ('\"name\"' in v[\"data\"])))\n",
    "        hotel_info = hotel_cache[\"locations\"][0]\n",
    "        info = parse_hotel_info(hotel_info)\n",
    "\n",
    "    # for reviews we first need to scrape multiple pages\n",
    "    # so, first let's find total amount of pages\n",
    "    total_reviews = hotel_info[\"reviewSummary\"][\"count\"]\n",
    "    _review_page_size = 10\n",
    "    total_review_pages = int(math.ceil(total_reviews / _review_page_size))\n",
    "    log.info(f\"found {total_reviews} reviews in {total_review_pages} pages, taking first one\")\n",
    "    # then we can scrape all review pages one by one\n",
    "    # note: in review url \"or\" stands for \"offset reviews\"\n",
    "    review_urls = [\n",
    "        url.replace(\"-Reviews-\", f\"-Reviews-or{_review_page_size * i}-\") for i in range(2, total_review_pages + 1)\n",
    "    ]\n",
    "    assert len(set(review_urls)) == len(review_urls)\n",
    "    reviews = parse_reviews(first_page)\n",
    "    n = 0\n",
    "    for url in review_urls:\n",
    "        if n > 80: break\n",
    "        result = session.scrape(ScrapeConfig(url=url, country=\"US\"))\n",
    "        try:\n",
    "            reviews.extend(parse_reviews(result))\n",
    "        except:\n",
    "            continue\n",
    "        n += 1\n",
    "    log.info(f\"{len(reviews)} reviews scraped\")\n",
    "\n",
    "    return {\n",
    "        \"hotel\": hotel,\n",
    "        \"url\": url,\n",
    "        \"price\": pricing_details,\n",
    "        \"info\": info,\n",
    "        \"reviews\": reviews,\n",
    "        \"page\": page_data\n",
    "    }\n",
    "\n",
    "\n",
    "def es_insert(result_hotel):\n",
    "    # Generate a sha256 value from `url` for Elasticsearch doc id\n",
    "    m = hashlib.sha256()\n",
    "    m.update(result_hotel['url'].encode())\n",
    "    pkey = m.hexdigest()\n",
    "    \n",
    "    for key, val in result_hotel.items():\n",
    "        # Add/refresh `val` to Elasticsearch with index `key`\n",
    "        if key == 'url': continue\n",
    "        elif key == 'hotel':\n",
    "            doc = {key: val, 'url': result_hotel['url']}\n",
    "            res = es.index(index=key, id=pkey, document=doc)\n",
    "        elif key == 'price':\n",
    "            res = es.index(index=key, id=pkey, document={\"group\": \"price\", \"price\": val})\n",
    "        elif key == 'reviews':\n",
    "            res = es.index(index=key, id=pkey, document={\"group\": \"reviews\", \"reviews\": val})\n",
    "        elif key == 'page':\n",
    "            res = es.index(index=key, id=pkey, document={\"group\": \"page\",\n",
    "                                                         \"page\": val['urqlCache']})\n",
    "                                                         #\"page\": val})\n",
    "        else:\n",
    "            res = es.index(index=key, id=pkey, document=val)\n",
    "        print(f\"Added document as index '{key}'\")\n",
    "        print(res['result'])\n",
    "\n",
    "\n",
    "def pg_insert(conn, result_hotel):\n",
    "    # Insert the `url` and `result_hotel` into PostgreSQL\n",
    "    with conn.cursor() as cur:\n",
    "        sql = \"\"\"INSERT INTO hotels (hotel, url, info, price, reviews, page) \n",
    "                   VALUES (%s, %s, %s, %s, %s, %s);\"\"\"\n",
    "        cur.execute(sql, (result_hotel['hotel'],\n",
    "                          result_hotel['url'],\n",
    "                          extras.Json(result_hotel['info']),\n",
    "                          extras.Json(result_hotel['price']),\n",
    "                          extras.Json(result_hotel['reviews']),\n",
    "                          extras.Json(result_hotel['page'])))\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "def run(hotel, query):\n",
    "    secrets_scrapfly = hidden.scrapfly()\n",
    "    key = secrets_scrapfly['key']\n",
    "    with ScrapflyClient(key=key, max_concurrency=20) as session:\n",
    "        hotel_url = \"https://www.tripadvisor.com/\" + (search_hotel(query, session))[\"url\"]\n",
    "        result_hotel = scrape_hotel(\n",
    "            hotel,\n",
    "            hotel_url,\n",
    "            session,\n",
    "        )\n",
    "\n",
    "        return result_hotel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape missing hotel in `ta_hotels` from TripAdvisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-07 18:59:53.972 | INFO     | __main__:search_hotel:5 - searching: Wentworth Mansion, Charleston, United States\n",
      "2022-10-07 18:59:56.432 | INFO     | __main__:search_hotel:85 - found 2 results, taking the most matched result\n",
      "2022-10-07 18:59:56.434 | INFO     | __main__:scrape_hotel:191 - scraping: https://www.tripadvisor.com//Hotel_Review-g54171-d111475-Reviews-Wentworth_Mansion-Charleston_South_Carolina.html\n",
      "2022-10-07 18:59:59.520 | INFO     | __main__:scrape_hotel:210 - found 3 prices from ['Expedia.com', 'Hotels.com', 'Official Site']\n",
      "2022-10-07 18:59:59.521 | INFO     | __main__:scrape_hotel:229 - found 1212 reviews in 122 pages, taking first one\n",
      "2022-10-07 19:02:34.371 | INFO     | __main__:scrape_hotel:246 - 820 reviews scraped\n",
      "WARNING:elastic_transport.node_pool:Node <Urllib3HttpNode(https://es.matrix4p.com:9200)> has failed for 1 times in a row, putting on 1 second timeout\n",
      "WARNING:elastic_transport.node_pool:Node <Urllib3HttpNode(https://es.matrix4p.com:9200)> has been marked alive after a successful request\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added document as index 'hotel'\n",
      "created\n",
      "Added document as index 'price'\n",
      "created\n",
      "Added document as index 'info'\n",
      "created\n",
      "Added document as index 'reviews'\n",
      "created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-07 19:02:44.973 | INFO     | __main__:search_hotel:5 - searching: Taj Lands End, Mumbai, India\n",
      "2022-10-07 19:02:46.285 | INFO     | __main__:search_hotel:85 - found 6 results, taking the most matched result\n",
      "2022-10-07 19:02:46.286 | INFO     | __main__:scrape_hotel:191 - scraping: https://www.tripadvisor.com//Hotel_Review-g304554-d304604-Reviews-Taj_Lands_End_Mumbai-Mumbai_Maharashtra.html\n",
      "2022-10-07 19:02:48.938 | INFO     | __main__:scrape_hotel:210 - found 3 prices from ['Official Site', 'Expedia.com', 'ZenHotels.com']\n",
      "2022-10-07 19:02:48.940 | INFO     | __main__:scrape_hotel:229 - found 5916 reviews in 592 pages, taking first one\n",
      "2022-10-07 19:05:42.148 | INFO     | __main__:scrape_hotel:246 - 820 reviews scraped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added document as index 'hotel'\n",
      "created\n",
      "Added document as index 'price'\n",
      "created\n",
      "Added document as index 'info'\n",
      "created\n",
      "Added document as index 'reviews'\n",
      "created\n",
      "Added document as index 'hotel'\n",
      "updated\n",
      "Added document as index 'price'\n",
      "updated\n",
      "Added document as index 'info'\n",
      "updated\n",
      "Added document as index 'reviews'\n",
      "updated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-07 19:05:42.840 | INFO     | __main__:search_hotel:5 - searching: Birkenhead House, Hermanus, South Africa\n",
      "2022-10-07 19:05:44.791 | INFO     | __main__:search_hotel:85 - found 2 results, taking the most matched result\n",
      "2022-10-07 19:05:44.792 | INFO     | __main__:scrape_hotel:191 - scraping: https://www.tripadvisor.com//Hotel_Review-g312663-d459781-Reviews-Birkenhead_House-Hermanus_Overstrand_Overberg_District_Western_Cape.html\n",
      "2022-10-07 19:05:46.984 | INFO     | __main__:scrape_hotel:210 - found 6 prices from ['Hotels.com', 'Expedia.com', 'Travelocity', 'Priceline', 'Orbitz.com', 'eDreams']\n",
      "2022-10-07 19:05:46.985 | INFO     | __main__:scrape_hotel:229 - found 528 reviews in 53 pages, taking first one\n",
      "2022-10-07 19:07:31.373 | INFO     | __main__:scrape_hotel:246 - 488 reviews scraped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added document as index 'hotel'\n",
      "created\n",
      "Added document as index 'price'\n",
      "created\n",
      "Added document as index 'info'\n",
      "created\n",
      "Added document as index 'reviews'\n",
      "created\n",
      "Added document as index 'hotel'\n",
      "updated\n",
      "Added document as index 'price'\n",
      "updated\n",
      "Added document as index 'info'\n",
      "updated\n",
      "Added document as index 'reviews'\n",
      "updated\n"
     ]
    }
   ],
   "source": [
    "# Load the Travel + Leisure World's Best Hotels 2022 dataset\n",
    "hotels_2022_df = pd.read_csv('./data/100_hotels_2022.csv', encoding='latin1')\n",
    "\n",
    "# Load scraped TripAdvisor hotels info dataset\n",
    "ta_hotels_df = pd.read_pickle('./data/ta_hotels.pickle.zip')\n",
    "\n",
    "for hotel in hotels_2022_df.Hotel:\n",
    "    #if not any(hotel.lower() in str.lower(row['name']) for row in ta_hotels_df['info']):  # check if hotel already scraped\n",
    "    if not hotel in ta_hotels_df.hotel.tolist():  # check if hotel already scraped\n",
    "        query = hotel \\\n",
    "            + ', ' + hotels_2022_df.loc[hotels_2022_df.Hotel == hotel, 'Location'].values[0] \\\n",
    "            + ', ' + hotels_2022_df.loc[hotels_2022_df.Hotel == hotel, 'Country'].values[0]\n",
    "        result_hotel = run(hotel, query)\n",
    "\n",
    "        # Insert result to Elasticsearch\n",
    "        try:\n",
    "            es_insert(result_hotel=result_hotel)\n",
    "        except:\n",
    "            try: \n",
    "                es_insert(result_hotel=result_hotel)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # Insert result to PostgreSQL database\n",
    "        pg_insert(conn=conn, result_hotel=result_hotel)\n",
    "\n",
    "        # Insert result to dataframe and save to pickle file\n",
    "        row = pd.Series(result_hotel).to_frame().T\n",
    "        ta_hotels_df = pd.concat([ta_hotels_df, row], ignore_index=True)\n",
    "        ta_hotels_df.to_pickle('./data/ta_hotels.pickle.zip')\n",
    "        \n",
    "        #break\n",
    "    else: print('All hotels are scraped and stored')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape one hotel link and insert to databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the hotel to scrape\n",
    "hotels_2022_df.iloc[60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape the link of the hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.tripadvisor.com//Hotel_Review-g187147-d497267-Reviews-or550-La_Reserve_Paris_Hotel_and_Spa-Paris_Ile_de_France.html'\n",
    "\n",
    "secrets_scrapfly = hidden.scrapfly()\n",
    "key = secrets_scrapfly['key']\n",
    "with ScrapflyClient(key=key, max_concurrency=20) as session:\n",
    "    hotel_url = url\n",
    "    result_hotel = scrape_hotel(\n",
    "        hotel,\n",
    "        hotel_url,\n",
    "        session,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert the result to Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Elasticsearch connection\n",
    "secrets = hidden.elastic()\n",
    "\n",
    "es = Elasticsearch(\n",
    "    f\"https://{secrets['host']}:{secrets['port']}\",\n",
    "    ca_certs=secrets['ca'],\n",
    "    basic_auth=(secrets['user'], secrets['pass']),\n",
    "    verify_certs=False\n",
    ")\n",
    "\n",
    "# Insert result to Elasticsearch\n",
    "try:  # try/except in case the connection to Elasticsearch got lost\n",
    "    es_insert(result_hotel=result_hotel)\n",
    "except:\n",
    "    try: # try/except to pass th mapping error\n",
    "        es_insert(result_hotel=result_hotel)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert the result to PostgreSQL and save to Pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up PostgreSQL connection\n",
    "sql_string = hidden.psycopg2(hidden.postgres())\n",
    "conn = psycopg2.connect(sql_string, connect_timeout=3)\n",
    "\n",
    "# Insert result to PostgreSQL database\n",
    "pg_insert(conn=conn, result_hotel=result_hotel)\n",
    "\n",
    "# Insert result to df\n",
    "row = pd.Series(result_hotel).to_frame().T\n",
    "ta_hotels_df = pd.concat([ta_hotels_df, row], ignore_index=True)\n",
    "ta_hotels_df.to_pickle('./data/ta_hotels.pickle.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve one hotel pricing and insert to databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape the link of the hotel and retrieve prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.tripadvisor.com//Hotel_Review-g60763-d93585-Reviews-or480-Lowell_Hotel-New_York_City_New_York.html\n"
     ]
    }
   ],
   "source": [
    "# Load scraped TripAdvisor hotels info dataset\n",
    "ta_hotels_df = pd.read_pickle('./data/ta_hotels.pickle.zip')\n",
    "\n",
    "hotel = 'The Lowell'\n",
    "\n",
    "url = ta_hotels_df.loc[ta_hotels_df.hotel == hotel, 'url'].values[0]\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-14 11:48:09.435 | INFO     | __main__:<module>:5 - scraping: https://www.tripadvisor.com//Hotel_Review-g60763-d93585-Reviews-or480-Lowell_Hotel-New_York_City_New_York.html\n",
      "2022-10-14 11:48:14.670 | INFO     | __main__:<module>:24 - found 0 prices from []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "secrets_scrapfly = hidden.scrapfly()\n",
    "key = secrets_scrapfly['key']\n",
    "with ScrapflyClient(key=key, max_concurrency=20) as session:\n",
    "    \"\"\"Scrape hotel pricing\"\"\"\n",
    "    log.info(f\"scraping: {url}\")\n",
    "    first_page = session.scrape(ScrapeConfig(url=url, country=\"US\"))\n",
    "\n",
    "page_data = extract_page_manifest(first_page.content)\n",
    "\n",
    "# price data keys are dynamic first we need to find the full key name\n",
    "_pricing_key = next(\n",
    "    (key for key, val in page_data['urqlCache'].items() if 'HPS_getWebHROffers' in val['data'])\n",
    ")\n",
    "pricing_details = list()\n",
    "price_page = json.loads(page_data['urqlCache'][_pricing_key]['data'])['HPS_getWebHROffers']\n",
    "for offer_key in ['chevronOffers', 'textLinkOffers']:\n",
    "    if any([source['status'] == 'AVAILABLE' for source in price_page[offer_key]]):\n",
    "        for source in price_page[offer_key]:\n",
    "            if source['status'] == 'AVAILABLE':\n",
    "                pricing = dict()\n",
    "                pricing['vendorName'] = source['data']['dataAtts']['vendorName']\n",
    "                pricing['perNight'] = source['data']['dataAtts']['perNight']\n",
    "                pricing_details.append(pricing)\n",
    "\n",
    "log.info(f\"found {len(pricing_details)} prices from {[pricing['vendorName'] for pricing in pricing_details]}\")\n",
    "\n",
    "print(pricing_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post the payload to TripAdvisor and Scrape the prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.tripadvisor.com//Hotel_Review-g667237-d2104242-Reviews-or600-Secret_Bay-Portsmouth_Saint_John_Parish_Dominica.html\n",
      "2104242\n"
     ]
    }
   ],
   "source": [
    "# Load scraped TripAdvisor hotels info dataset\n",
    "ta_hotels_df = pd.read_pickle('./data/ta_hotels.pickle.zip')\n",
    "\n",
    "hotel = 'Secret Bay'\n",
    "\n",
    "url = ta_hotels_df.loc[ta_hotels_df.hotel == hotel, 'url'].values[0]\n",
    "print(url)\n",
    "\n",
    "page_data = ta_hotels_df.loc[ta_hotels_df.hotel == hotel, 'page'].values[0]\n",
    "hotel_cache = extract_named_urql_cache(page_data[\"urqlCache\"], '\"locationDescription\"')\n",
    "hotel_info = hotel_cache[\"locations\"][0]\n",
    "info = parse_hotel_info(hotel_info)\n",
    "\n",
    "hotelId = info['id']\n",
    "print(hotelId)\n",
    "\n",
    "checkInDate = \"2023-05-07\"\n",
    "checkOutDate = \"2023-05-11\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-14 16:08:44.929 | INFO     | __main__:<module>:45 - scraping: https://www.tripadvisor.com//Hotel_Review-g667237-d2104242-Reviews-or600-Secret_Bay-Portsmouth_Saint_John_Parish_Dominica.html\n"
     ]
    }
   ],
   "source": [
    "payload = json.dumps(\n",
    "  [\n",
    "    {\n",
    "      \"query\": \"eaea6d7eb0208f01fe0ca7d27238cbc1\",\n",
    "      \"variables\": {\n",
    "        \"request\": {\n",
    "          \"hotelId\": hotelId,\n",
    "          \"trackingEnabled\": True,\n",
    "          \"requestCaller\": \"Hotel_Review\",\n",
    "          \"impressionPlacement\": \"HR_MainCommerce\",\n",
    "          \"pageLoadUid\": \"4c234841-fa05-48dd-9cab-13255d6ecb30\",\n",
    "          \"sessionId\": \"FBD040A0F2094BB8BEAE17EDA8C00A45\",\n",
    "          \"currencyCode\": \"USD\",\n",
    "          \"requestNumber\": 1,\n",
    "          \"shapeStrategy\": \"DEFAULT_DESKTOP_OFFER_SHAPE\",\n",
    "          \"optimusEnabled\": True,\n",
    "          \"sequenceId\": 0,\n",
    "          \"travelInfo\": {\n",
    "            \"checkInDate\": checkInDate,\n",
    "            \"checkOutDate\": checkOutDate,\n",
    "            \"usedDefaultDates\": False,\n",
    "            \"rooms\": 1,\n",
    "            \"adults\": 1,\n",
    "            \"childAgesPerRoom\": []\n",
    "          },\n",
    "          \"allowOptimusDisplayPrice\": False\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "\n",
    "query_url = 'https://www.tripadvisor.com/data/graphql/ids'\n",
    "\n",
    "headers = {\n",
    "    # we need to generate a random request ID for this request to succeed\n",
    "    \"content-type\": \"application/json\",\n",
    "    \"x-requested-by\": \"\".join(random.choice(string.ascii_lowercase + string.digits) for i in range(64)),\n",
    "}\n",
    "\n",
    "secrets_scrapfly = hidden.scrapfly()\n",
    "key = secrets_scrapfly['key']\n",
    "with ScrapflyClient(key=key, max_concurrency=20) as session:\n",
    "    \"\"\"Scrape hotel pricing\"\"\"\n",
    "    log.info(f\"scraping: {url}\")\n",
    "    result = session.scrape(\n",
    "        ScrapeConfig(\n",
    "            url=query_url,\n",
    "            country=\"US\",\n",
    "            headers=headers,\n",
    "            body=payload,\n",
    "            method=\"POST\",\n",
    "            asp=True,\n",
    "        )\n",
    "    )\n",
    "    data = json.loads(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-14 16:08:49.442 | INFO     | __main__:<module>:12 - found 2 prices from ['Booking.com', 'eDreams']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'vendorName': 'Booking.com', 'perNight': 1578}, {'vendorName': 'eDreams', 'perNight': 1578}]\n"
     ]
    }
   ],
   "source": [
    "pricing_details = list()\n",
    "price_page = data[0]['data']['HPS_getWebHROffers']\n",
    "for offer_key in ['chevronOffers', 'textLinkOffers']:\n",
    "    if any([source['status'] == 'AVAILABLE' for source in price_page[offer_key]]):\n",
    "        for source in price_page[offer_key]:\n",
    "            if source['status'] == 'AVAILABLE':\n",
    "                pricing = dict()\n",
    "                pricing['vendorName'] = source['data']['dataAtts']['vendorName']\n",
    "                pricing['perNight'] = source['data']['dataAtts']['perNight']\n",
    "                pricing_details.append(pricing)\n",
    "\n",
    "log.info(f\"found {len(pricing_details)} prices from {[pricing['vendorName'] for pricing in pricing_details]}\")\n",
    "\n",
    "print(pricing_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.tripadvisor.com//Hotel_Review-g667237-d2104242-Reviews-or600-Secret_Bay-Portsmouth_Saint_John_Parish_Dominica.html\n",
      "Secret Bay\n"
     ]
    }
   ],
   "source": [
    "print(url)\n",
    "print(hotel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert the result to Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added document as index 'price'\n",
      "created\n"
     ]
    }
   ],
   "source": [
    "# Set up Elasticsearch connection\n",
    "secrets = hidden.elastic()\n",
    "\n",
    "es = Elasticsearch(\n",
    "    f\"https://{secrets['host']}:{secrets['port']}\",\n",
    "    ca_certs=secrets['ca'],\n",
    "    basic_auth=(secrets['user'], secrets['pass']),\n",
    "    verify_certs=False\n",
    ")\n",
    "\n",
    "# Generate a sha256 value from `url` for Elasticsearch doc id\n",
    "m = hashlib.sha256()\n",
    "m.update(url.encode())\n",
    "pkey = m.hexdigest()\n",
    "\n",
    "# Insert `pricing_details` to Elasticsearch\n",
    "res = es.index(index=key, id=pkey, document={\"group\": \"price\", \"price\": pricing_details})\n",
    "print(f\"Added document as index 'price'\")\n",
    "print(res['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert the result to PostgreSQL and save to Pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up PostgreSQL connection\n",
    "sql_string = hidden.psycopg2(hidden.postgres())\n",
    "conn = psycopg2.connect(sql_string, connect_timeout=3)\n",
    "\n",
    "# Insert result to PostgreSQL database\n",
    "with conn.cursor() as cur:\n",
    "    sql = \"\"\"UPDATE hotels\n",
    "               SET price = %s \n",
    "               WHERE hotel = %s;\"\"\"\n",
    "    cur.execute(sql, (extras.Json(pricing_details), hotel))\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "# Insert result to df\n",
    "ta_hotels_df.at[ta_hotels_df[ta_hotels_df.hotel == hotel].index.values[0], 'price'] = pricing_details\n",
    "ta_hotels_df.to_pickle('./data/ta_hotels.pickle.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refresh Database Indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recreate PostgreSQL Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up PostgreSQL connection\n",
    "sql_string = hidden.psycopg2(hidden.postgres())\n",
    "conn = psycopg2.connect(sql_string, connect_timeout=3)\n",
    "\n",
    "# Create indices for `hotels` table\n",
    "columns = ['hotel', 'url', 'info', 'price', 'reviews', 'page']\n",
    "with conn.cursor() as cur:\n",
    "    for column in columns:\n",
    "        if column in ['hotel', 'url']:\n",
    "            continue\n",
    "        else:\n",
    "            sql = f\"DROP INDEX IF EXISTS {column}_gin;\"\n",
    "            cur.execute(sql)\n",
    "            sql = f\"CREATE INDEX {column}_gin ON hotels USING gin ({column});\"\n",
    "            cur.execute(sql)\n",
    "\n",
    "            sql = f\"DROP INDEX IF EXISTS {column}_gin_path_ops;\"\n",
    "            cur.execute(sql)\n",
    "            sql = f\"CREATE INDEX {column}_gin_path_ops ON hotels USING gin ({column} jsonb_path_ops);\"\n",
    "            cur.execute(sql)\n",
    "conn.commit()\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refresh Elasticsearch Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Elasticsearch connection\n",
    "secrets = hidden.elastic()\n",
    "\n",
    "es = Elasticsearch(\n",
    "    f\"https://{secrets['host']}:{secrets['port']}\",\n",
    "    ca_certs=secrets['ca'],\n",
    "    basic_auth=(secrets['user'], secrets['pass']),\n",
    "    verify_certs=False\n",
    ")\n",
    "\n",
    "# Tell it to recompute the Elasticsearch indices - normally it would take up to 30 seconds\n",
    "for column in columns:\n",
    "    if column == 'url': continue\n",
    "    res = es.indices.refresh(index=column)\n",
    "    print(f\"Index {column} refreshed\")\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record Library Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: 2022-10-09T08:46:23.211373+08:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.9.13\n",
      "IPython version      : 8.5.0\n",
      "\n",
      "loguru       : 0.6.0\n",
      "scrapfly     : 0.8.2\n",
      "elasticsearch: 8.4.2\n",
      "\n",
      "pandas  : 1.5.0\n",
      "re      : 2.2.1\n",
      "json    : 2.0.9\n",
      "sys     : 3.9.13 | packaged by conda-forge | (main, May 27 2022, 17:00:52) \n",
      "[Clang 13.0.1 ]\n",
      "psycopg2: 2.9.3\n",
      "\n",
      "Watermark: 2.3.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -u -i -d -v -iv -w -p loguru,scrapfly,elasticsearch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
